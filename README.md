# Bias and Retrieval Experiments (BBQ + NQ)

This repository contains code and results for a series of experiments probing whether identity-bearing language shifts semantic similarity, retrieval, and generation in large language models (LLMs) and embedding models.

---

 
---

## Research Questions

| Experiment ID | Research Question |
|---|---|
| **BBQ-RQ1** | Is there variability in cosine similarity across ambiguous contexts for the same question? | 
| **BBQ-RQ2a** | Does disambiguation increase cosine similarity with the question? | 
| **BBQ-RQ2b** | Does disambiguation increase similarity more when it counters stereotypes? | 
| **BBQ-RQ3** | Do negative questions align more with stereotype-confirming contexts than non-negative ones? |
| **NQ-RQ1** | Does injecting an identity into an answer passage shift its similarity to a neutral question? Is the shift identity- or position-dependent?  |
| **NQ-RQ2** | Does identity injection in prompts/queries affect retrieval/ranking? |
| **NQ-RQ3** | Does identity injection in prompts/queries affect generated outputs? |

---


## Input Data


---

## Expected Ouputs




---

## Running the Experiments



---


## Dependencies
Install requirements:
```bash
pip install -r requirements.txt
